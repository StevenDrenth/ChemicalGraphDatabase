{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDnTqXKu3kgu"
      },
      "source": [
        "# GNN with Pytorch Geometric\n",
        "In this notebook, an existing dataloader of PyG, the TUDataset, will be adapted to work with our files. Afterwards, the loaded data will be tested with a simple graph neural network (GNN). </br>\n",
        "Note that, due to problems with the local environment and pytorch, this notebook was tested and developed to work in google colab. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v79KVlF3wJl"
      },
      "source": [
        "## Install and import necesarry pytorch packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUc6cGHsPCGf",
        "outputId": "0d7ad4d8-589d-42e9-e028-0adfdd4e721b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.12.1+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 2.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 2.6 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_sparse import coalesce\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.io import read_txt_array\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "from torch_geometric.data import InMemoryDataset, download_url, extract_zip\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "from IPython.display import Javascript\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "import shutil\n",
        "from typing import Callable, List, Optional\n",
        "\n",
        "import networkx as nx\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05OkPtkI4J19"
      },
      "source": [
        "## Adapt the TUDataset dataloader to load own data\n",
        "The structure of the TUDataset dataloader code is useful, but some changes are made to it in order to be able to work with own datafiles. Additionally, the code is completely commented to improve readability in comparison to the almost uncommented original. <br>\n",
        "Original code: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/tu_dataset.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### read_data function\n",
        "First, the read_data function will be determined, which is used by the dataloader to read the data from the given .txt file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti4NWWEtj2r8"
      },
      "outputs": [],
      "source": [
        "# Import necesarry packages\n",
        "import glob\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_sparse import coalesce\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.io import read_txt_array\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "\n",
        "def read_data(folder, prefix):\n",
        "    \"\"\"\n",
        "    This function reads the given text files and depending on the name,\n",
        "    it will be processed accordingly. This code is based on the read_tu_data\n",
        "    code from torch_geometric.io and adapted to work with non-TU datasets.\n",
        "    \"\"\"\n",
        "  # Load the .txt files from the specified folder\n",
        "    files = glob.glob(osp.join(folder, f'{prefix}_*.txt'))\n",
        "  # For every file in the folder, put the name in a list\n",
        "    names = [f.split(os.sep)[-1][len(prefix) + 1:-4] for f in files]\n",
        "\n",
        "  # Read the adjacency matrix .txt file\n",
        "    edge_index = read_file(folder, prefix, 'A', torch.long).t() - 1\n",
        "  # Read the graph ID .txt file\n",
        "    batch = read_file(folder, prefix, 'graph_indicator', torch.long) - 1\n",
        "\n",
        "  # Read the node atrribute .txt file (if it's in the folder)\n",
        "    node_attributes = torch.empty((batch.size(0), 0))\n",
        "    if 'node_attributes' in names:\n",
        "        node_attributes = read_file(folder, prefix, 'node_attributes')\n",
        "\n",
        "  # Read the node labels .txt file (if it's in the folder)\n",
        "    node_labels = torch.empty((batch.size(0), 0))\n",
        "    if 'node_labels' in names:\n",
        "        node_labels = read_file(folder, prefix, 'node_labels', torch.long)\n",
        "      # If the dimension is 1, unsqueeze the tensor to seperate the labels\n",
        "        if node_labels.dim() == 1:\n",
        "            node_labels = node_labels.unsqueeze(-1)\n",
        "      # If applicable, downsize \n",
        "        node_labels = node_labels - node_labels.min(dim=0)[0]\n",
        "      # Create individual tensors\n",
        "        node_labels = node_labels.unbind(dim=-1)\n",
        "      # One-hot encode the labels\n",
        "        node_labels = [F.one_hot(x, num_classes=-1) for x in node_labels]\n",
        "      # Change to float\n",
        "        node_labels = torch.cat(node_labels, dim=-1).to(torch.float)\n",
        "\n",
        "  # Read the edge attributes .txt file (if it's in the folder)\n",
        "    edge_attributes = torch.empty((edge_index.size(1), 0))\n",
        "    if 'edge_attributes' in names:\n",
        "        edge_attributes = read_file(folder, prefix, 'edge_attributes')\n",
        "\n",
        "  # Read the edge labels .txt file (if it's in the folder)\n",
        "    edge_labels = torch.empty((edge_index.size(1), 0))\n",
        "    if 'edge_labels' in names:\n",
        "        edge_labels = read_file(folder, prefix, 'edge_labels', torch.long)\n",
        "      # If the dimension is 1, unsqueeze the tensor to seperate the labels\n",
        "        if edge_labels.dim() == 1:\n",
        "            edge_labels = edge_labels.unsqueeze(-1)\n",
        "      # If applicable, downsize\n",
        "        edge_labels = edge_labels - edge_labels.min(dim=0)[0]\n",
        "      # Create individual tensors\n",
        "        edge_labels = edge_labels.unbind(dim=-1)\n",
        "      # One-hot encode the labels\n",
        "        edge_labels = [F.one_hot(e, num_classes=-1) for e in edge_labels]\n",
        "      # Change to float\n",
        "        edge_labels = torch.cat(edge_labels, dim=-1).to(torch.float)\n",
        "\n",
        "  # Create the x variable using the node attributes and labels\n",
        "    x = cat([node_attributes, node_labels])\n",
        "  # Create the edge_attr variable by using the egde attributes and labels\n",
        "    edge_attr = cat([edge_attributes, edge_labels])\n",
        "\n",
        "  # Set the y variable to None\n",
        "    y = None\n",
        "  # If there are graph attributes or graph labels, this will be used as the y\n",
        "  # variable\n",
        "    if 'graph_attributes' in names:  # Regression problem.\n",
        "        y = read_file(folder, prefix, 'graph_attributes')\n",
        "    elif 'graph_labels' in names:  # Classification problem.\n",
        "        y = read_file(folder, prefix, 'graph_labels', torch.long)\n",
        "        _, y = y.unique(sorted=True, return_inverse=True)\n",
        "\n",
        "  # Count the number of nodes\n",
        "    num_nodes = edge_index.max().item() + 1 if x is None else x.size(0)\n",
        "  # Remove the self loops\n",
        "    edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
        "  # Coalesce the edge index and attribute\n",
        "    edge_index, edge_attr = coalesce(edge_index, edge_attr, num_nodes,\n",
        "                                     num_nodes)\n",
        "\n",
        "  # Create the dataset\n",
        "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
        "  # Create the slices the indicate the separations between graphs\n",
        "    data, slices = split(data, batch)\n",
        "\n",
        "  # Calculate sizes\n",
        "    sizes = {\n",
        "        'num_node_attributes': node_attributes.size(-1),\n",
        "        'num_node_labels': node_labels.size(-1),\n",
        "        'num_edge_attributes': edge_attributes.size(-1),\n",
        "        'num_edge_labels': edge_labels.size(-1),\n",
        "    }\n",
        "\n",
        "    return data, slices, sizes\n",
        "\n",
        "\n",
        "def read_file(folder, prefix, name, dtype=None):\n",
        "    \"\"\"\n",
        "    Read a file in the specified path, return as specified type\n",
        "    \"\"\"\n",
        "    path = osp.join(folder, f'{prefix}_{name}.txt')\n",
        "    return read_txt_array(path, sep=',', dtype=dtype)\n",
        "\n",
        "\n",
        "def cat(seq):\n",
        "    \"\"\"\n",
        "    Combine the given input in one tensor\n",
        "    \"\"\"\n",
        "    seq = [item for item in seq if item is not None]\n",
        "    seq = [item for item in seq if item.numel() > 0]\n",
        "    seq = [item.unsqueeze(-1) if item.dim() == 1 else item for item in seq]\n",
        "    return torch.cat(seq, dim=-1) if len(seq) > 0 else None\n",
        "\n",
        "\n",
        "def split(data, batch):\n",
        "    \"\"\"\n",
        "    Determines the slices of the data based on the given batch indicator\n",
        "    \"\"\"\n",
        "    node_slice = torch.cumsum(torch.from_numpy(np.bincount(batch)), 0)\n",
        "    node_slice = torch.cat([torch.tensor([0]), node_slice])\n",
        "\n",
        "    row, _ = data.edge_index\n",
        "    edge_slice = torch.cumsum(torch.from_numpy(np.bincount(batch[row])), 0)\n",
        "    edge_slice = torch.cat([torch.tensor([0]), edge_slice])\n",
        " \n",
        "    # Edge indices should start at zero for every graph.\n",
        "    data.edge_index -= node_slice[batch[row]].unsqueeze(0)\n",
        "\n",
        "    slices = {'edge_index': edge_slice}\n",
        "    if data.x is not None:\n",
        "        slices['x'] = node_slice\n",
        "    else:\n",
        "        # Imitate `collate` functionality:\n",
        "        data._num_nodes = torch.bincount(batch).tolist()\n",
        "        data.num_nodes = batch.numel()\n",
        "    if data.edge_attr is not None:\n",
        "        slices['edge_attr'] = edge_slice\n",
        "    if data.y is not None:\n",
        "        if data.y.size(0) == batch.size(0):\n",
        "            slices['y'] = node_slice\n",
        "        else:\n",
        "            btch = batch.sort().values # Sort values\n",
        "            slices['y'] = torch.arange(0, btch[-1] + 2, dtype=torch.long)\n",
        "\n",
        "    return data, slices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LoadData\n",
        "The dataloader, to which callable properties are added"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K-KBX6UIfRD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "import shutil\n",
        "from typing import Callable, List, Optional\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch_geometric.data import InMemoryDataset, download_url, extract_zip\n",
        "\n",
        "class LoadData(InMemoryDataset):\n",
        "  \"\"\"\n",
        "  Loads the data of the different .txt files, being:\n",
        "    Adjacency matrix: root + _A\n",
        "    Graph ID: root + _graph_indicator\n",
        "    Edge labels: root + _edge_labels\n",
        "    Node labels: root + _node_labels\n",
        "    Node attributes: root + _node_attributes\n",
        "  This is transformed to a workable data format for pytorch geometric\n",
        "\n",
        "  Note that it has to be specified if the node and edge attributes are used, as\n",
        "  well as the transform, pre-transform and pre-filter, the default is not using \n",
        "  them\n",
        "  \"\"\"\n",
        "  def __init__(self, root: str, name: str,\n",
        "              transform: Optional[Callable] = None,\n",
        "              pre_transform: Optional[Callable] = None,\n",
        "              pre_filter: Optional[Callable] = None,\n",
        "              use_node_attr: bool = False, use_edge_attr: bool = False,\n",
        "              cleaned: bool = False):\n",
        "    self.name = name\n",
        "    self.cleaned = cleaned\n",
        "    super().__init__(root, transform, pre_transform, pre_filter)\n",
        "\n",
        "  # If there is already a processed file available, this is loaded if not, created\n",
        "    out = torch.load(self.processed_paths[0])\n",
        "    if not isinstance(out, tuple) and len(out) != 3:\n",
        "        raise RuntimeError(\n",
        "            \"The 'data' object was created by an older version of PyG. \"\n",
        "            \"If this error occurred while loading an already existing \"\n",
        "            \"dataset, remove the 'processed/' directory in the dataset's \"\n",
        "            \"root folder and try again.\")\n",
        "    self.data, self.slices, self.sizes = out\n",
        "\n",
        "\n",
        "    if self.data.x is not None and not use_node_attr:\n",
        "        num_node_attributes = self.num_node_attributes\n",
        "        self.data.x = self.data.x[:, num_node_attributes:]\n",
        "    if self.data.edge_attr is not None and not use_edge_attr:\n",
        "        num_edge_attributes = self.num_edge_attributes\n",
        "        self.data.edge_attr = self.data.edge_attr[:, num_edge_attributes:]\n",
        "\n",
        "  @property\n",
        "  def raw_dir(self) -> str:\n",
        "      name = f'raw{\"_cleaned\" if self.cleaned else \"\"}'\n",
        "      return osp.join(self.root, self.name, name)\n",
        "\n",
        "  @property\n",
        "  def processed_dir(self) -> str:\n",
        "      name = f'processed{\"_cleaned\" if self.cleaned else \"\"}'\n",
        "      return osp.join(self.root, self.name, name)\n",
        "\n",
        "  @property\n",
        "  def num_node_labels(self) -> int:\n",
        "      return self.sizes['num_node_labels']\n",
        "\n",
        "  @property\n",
        "  def num_node_attributes(self) -> int:\n",
        "      return self.sizes['num_node_attributes']\n",
        "\n",
        "  @property\n",
        "  def num_edge_labels(self) -> int:\n",
        "      return self.sizes['num_edge_labels']\n",
        "\n",
        "  @property\n",
        "  def num_edge_attributes(self) -> int:\n",
        "      return self.sizes['num_edge_attributes']\n",
        "\n",
        "  @property\n",
        "  def raw_file_names(self) -> List[str]:\n",
        "      names = ['A', 'graph_indicator']\n",
        "      return [f'{self.name}_{name}.txt' for name in names]\n",
        "\n",
        "  @property\n",
        "  def processed_file_names(self) -> str:\n",
        "      return 'data.pt'\n",
        "\n",
        "\n",
        "  def process(self):\n",
        "    # Read the data using the determined read_data function\n",
        "      self.data, self.slices, sizes = read_data(self.raw_dir, self.name) \n",
        "\n",
        "      if self.pre_filter is not None or self.pre_transform is not None:\n",
        "          data_list = [self.get(idx) for idx in range(len(self))]\n",
        "\n",
        "          if self.pre_filter is not None:\n",
        "              data_list = [d for d in data_list if self.pre_filter(d)]\n",
        "\n",
        "          if self.pre_transform is not None:\n",
        "              data_list = [self.pre_transform(d) for d in data_list]\n",
        "\n",
        "          self.data, self.slices = self.collate(data_list)\n",
        "          self._data_list = None  # Reset cache.\n",
        "\n",
        "    # Save the created files\n",
        "      torch.save((self.data, self.slices, sizes), self.processed_paths[0])\n",
        "\n",
        "  def __repr__(self) -> str:\n",
        "    return f'{self.name}({len(self)})'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use the data loader\n",
        "Load the chosen files with the LoadData class. This dataset consists of only molecules, no reaction nodes, the exact description of these files can be found in notebook 2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi1wh-7EWoIg",
        "outputId": "6d88b5a7-7b02-4dc9-c501-e57877d89147"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "mol(467)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = LoadData(root='data', name='mol', use_node_attr = True, use_edge_attr = False)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show the features of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm_6Emga2r0Y",
        "outputId": "23633315-27a1-43c1-9ea8-89a4dd847a41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: mol(467):\n",
            "======================\n",
            "Number of graphs: 467\n",
            "Number of features: 42\n",
            "Number of classes: 2\n"
          ]
        }
      ],
      "source": [
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Show an example graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLN_f90N3IGW",
        "outputId": "6b5f935f-29c0-4b5f-b3e3-11843810c7de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data(edge_index=[2, 8], x=[5, 42], edge_attr=[8, 4], y=[1])\n",
            "==============================================================\n",
            "Number of nodes: 5\n",
            "Number of edges: 8\n",
            "Average node degree: 1.60\n",
            "Has isolated nodes: False\n",
            "Has self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ],
      "source": [
        "data = dataset[1]  # Get the first graph object.\n",
        "\n",
        "print(data)\n",
        "print('==============================================================')\n",
        "\n",
        "# Gather some statistics about the graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print a random molecule from the dataset as test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XPmO7g-DgVCm",
        "outputId": "b6f2d294-92e9-4876-8b94-f0a9d6622206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "[{'edge_index': tensor([[ 0,  0,  1,  1,  1,  2,  2,  2,  3,  3,  4,  4,  5,  5,  5,  6,  6,  7,\n",
            "          7,  8,  8,  9, 10, 10, 11, 12, 12, 13],\n",
            "        [ 2,  3,  3,  8,  9,  0,  7, 12,  0,  1, 10, 13,  6, 10, 11,  5, 12,  2,\n",
            "          8,  1,  7,  1,  4,  5,  5,  2,  6,  4]]), 'x': tensor([[0.1348, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3501, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0599, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2911, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0500, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0040, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0306, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1383, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0613, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2326, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0998, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0715, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3264, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1428, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
            "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]), 'edge_attr': tensor([[0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [0., 0., 0., 1.]]), 'y': tensor([0])}]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Draw a random molecule from the dataset and print the properties\n",
        "n = random.randint(0,len(dataset)-1)\n",
        "data = dataset[n]\n",
        "\n",
        "# Draw the molecule\n",
        "g = torch_geometric.utils.to_networkx(data, to_undirected=True)\n",
        "print(nx.draw(g))\n",
        "\n",
        "# Print molecule features\n",
        "print(data.edge_stores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k16EEVBb_Bg9"
      },
      "source": [
        "## Graph Neural Network\n",
        "In this part, we will test the loaded graph dataset by creating a basic GNN. The code below is broadly based on a tutorial of PyG: https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=HvhgQoO8Svw4.</br> The goal of this GNN is to classify whether molecules are part of a metabolic pathway with a specific end molecule or not.</br>\n",
        "### Train/test split\n",
        "First, the data will be split in a training and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk2VP61nB2vZ",
        "outputId": "c1976a09-faa5-4cc2-b201-6ccf70d00843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training graphs: 350\n",
            "Number of test graphs: 117\n"
          ]
        }
      ],
      "source": [
        "# Set a seed\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Call the shuffle function of the dataset\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "# Split the shuffled dataset in a train and test part\n",
        "train_dataset = dataset[:350]\n",
        "test_dataset = dataset[350:]\n",
        "\n",
        "# Print the number of graphs per dataset\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ1p3BtFlDCX",
        "outputId": "1e9d4eed-2878-4244-f89b-8d789e385530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of y: 21\n"
          ]
        }
      ],
      "source": [
        "# Count the number of y values in the train dataset\n",
        "cnt = 0\n",
        "for n in range(len(train_dataset)):\n",
        "  if train_dataset[n].y == torch.tensor([1]):\n",
        "    cnt += 1\n",
        "print('number of y:', cnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEaRtSGiYzaL",
        "outputId": "e97dfabe-1635-48d9-9ff1-da01ac37fd2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of y: 10\n"
          ]
        }
      ],
      "source": [
        "# Count the number of y values in the test dataset\n",
        "cnt = 0\n",
        "for n in range(len(test_dataset)):\n",
        "  if test_dataset[n].y == torch.tensor([1]):\n",
        "    cnt += 1\n",
        "print('number of y:', cnt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d58qZT0sLmva",
        "outputId": "a6ad5031-d97c-4993-ec71-df53b2402321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 2476], x=[1197, 42], edge_attr=[2476, 4], y=[64], batch=[1197], ptr=[65])\n",
            "\n",
            "Step 2:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 2364], x=[1143, 42], edge_attr=[2364, 4], y=[64], batch=[1143], ptr=[65])\n",
            "\n",
            "Step 3:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 2442], x=[1174, 42], edge_attr=[2442, 4], y=[64], batch=[1174], ptr=[65])\n",
            "\n",
            "Step 4:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 2362], x=[1140, 42], edge_attr=[2362, 4], y=[64], batch=[1140], ptr=[65])\n",
            "\n",
            "Step 5:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 2548], x=[1224, 42], edge_attr=[2548, 4], y=[64], batch=[1224], ptr=[65])\n",
            "\n",
            "Step 6:\n",
            "=======\n",
            "Number of graphs in the current batch: 30\n",
            "DataBatch(edge_index=[2, 974], x=[474, 42], edge_attr=[974, 4], y=[30], batch=[474], ptr=[31])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the train and test dataset in batches\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Show output of number of batches\n",
        "for step, data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}:')\n",
        "    print('=======')\n",
        "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "    print(data)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the GNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XALPAfGLPxDY",
        "outputId": "d6d7e24c-acbe-4946-82dd-83e10e025d90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(42, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Create the GCN class\n",
        "class GCN(torch.nn.Module):\n",
        "    # Define the hidden channels\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = GCN(hidden_channels=64)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run the GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "ryOlhWeHLm0C",
        "outputId": "b0bbe3d8-e272-4d4c-d0ac-ecb70d62992b"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001, Train Acc: 0.9400, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 002, Train Acc: 0.9400, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 003, Train Acc: 0.9400, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 004, Train Acc: 0.9400, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 005, Train Acc: 0.9400, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 006, Train Acc: 0.9400, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 007, Train Acc: 0.9400, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 008, Train Acc: 0.9400, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 009, Train Acc: 0.9400, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 010, Train Acc: 0.9400, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 011, Train Acc: 0.9400, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 012, Train Acc: 0.9400, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 013, Train Acc: 0.9343, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 014, Train Acc: 0.9343, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 015, Train Acc: 0.9400, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 016, Train Acc: 0.9429, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 017, Train Acc: 0.9371, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 018, Train Acc: 0.9400, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 019, Train Acc: 0.9343, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 020, Train Acc: 0.9429, Test Acc: 0.9316, ones: 0.20000000000000026\n",
            "Epoch: 021, Train Acc: 0.9400, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 022, Train Acc: 0.9429, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 023, Train Acc: 0.9400, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 024, Train Acc: 0.9400, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 025, Train Acc: 0.9457, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 026, Train Acc: 0.9486, Test Acc: 0.9316, ones: 0.20000000000000026\n",
            "Epoch: 027, Train Acc: 0.9543, Test Acc: 0.9316, ones: 0.20000000000000026\n",
            "Epoch: 028, Train Acc: 0.9571, Test Acc: 0.9316, ones: 0.20000000000000026\n",
            "Epoch: 029, Train Acc: 0.9600, Test Acc: 0.8889, ones: -0.30000000000000054\n",
            "Epoch: 030, Train Acc: 0.9571, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 031, Train Acc: 0.9600, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 032, Train Acc: 0.9600, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 033, Train Acc: 0.9600, Test Acc: 0.9060, ones: -0.1\n",
            "Epoch: 034, Train Acc: 0.9629, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 035, Train Acc: 0.9686, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 036, Train Acc: 0.9714, Test Acc: 0.9060, ones: -0.1\n",
            "Epoch: 037, Train Acc: 0.9629, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 038, Train Acc: 0.9743, Test Acc: 0.9060, ones: -0.1\n",
            "Epoch: 039, Train Acc: 0.9743, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 040, Train Acc: 0.9800, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 041, Train Acc: 0.9800, Test Acc: 0.9316, ones: 0.20000000000000026\n",
            "Epoch: 042, Train Acc: 0.9886, Test Acc: 0.9316, ones: 0.20000000000000026\n",
            "Epoch: 043, Train Acc: 0.9857, Test Acc: 0.9316, ones: 0.20000000000000026\n",
            "Epoch: 044, Train Acc: 0.9743, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 045, Train Acc: 0.9686, Test Acc: 0.9060, ones: -0.1\n",
            "Epoch: 046, Train Acc: 0.9857, Test Acc: 0.9060, ones: -0.1\n",
            "Epoch: 047, Train Acc: 0.9743, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 048, Train Acc: 0.9829, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 049, Train Acc: 0.9857, Test Acc: 0.9316, ones: 0.20000000000000026\n",
            "Epoch: 050, Train Acc: 0.9914, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 051, Train Acc: 0.9886, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 052, Train Acc: 0.9743, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 053, Train Acc: 0.9714, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 054, Train Acc: 0.9886, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 055, Train Acc: 0.9943, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 056, Train Acc: 0.9914, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 057, Train Acc: 1.0000, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 058, Train Acc: 0.9914, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 059, Train Acc: 0.9971, Test Acc: 0.9316, ones: 0.20000000000000026\n",
            "Epoch: 060, Train Acc: 0.9971, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 061, Train Acc: 1.0000, Test Acc: 0.9316, ones: 0.20000000000000026\n",
            "Epoch: 062, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 063, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 064, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 065, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 066, Train Acc: 1.0000, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 067, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 068, Train Acc: 1.0000, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 069, Train Acc: 0.9971, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 070, Train Acc: 0.9886, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 071, Train Acc: 0.9914, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 072, Train Acc: 0.9629, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 073, Train Acc: 0.9657, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 074, Train Acc: 0.9714, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 075, Train Acc: 0.9857, Test Acc: 0.9573, ones: 0.5000000000000004\n",
            "Epoch: 076, Train Acc: 0.9886, Test Acc: 0.9573, ones: 0.5000000000000004\n",
            "Epoch: 077, Train Acc: 0.9771, Test Acc: 0.9145, ones: -3.552713678800501e-16\n",
            "Epoch: 078, Train Acc: 0.9914, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 079, Train Acc: 0.9857, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 080, Train Acc: 0.9914, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 081, Train Acc: 0.9943, Test Acc: 0.9573, ones: 0.5000000000000004\n",
            "Epoch: 082, Train Acc: 0.9943, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 083, Train Acc: 0.9857, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 084, Train Acc: 0.9971, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 085, Train Acc: 0.9971, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 086, Train Acc: 0.9971, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 087, Train Acc: 0.9971, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 088, Train Acc: 0.9971, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 089, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 090, Train Acc: 0.9914, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 091, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 092, Train Acc: 0.9971, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 093, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 094, Train Acc: 0.9971, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 095, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 096, Train Acc: 0.9857, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 097, Train Acc: 0.9914, Test Acc: 0.9231, ones: 0.10000000000000053\n",
            "Epoch: 098, Train Acc: 0.9857, Test Acc: 0.8974, ones: -0.19999999999999965\n",
            "Epoch: 099, Train Acc: 0.9943, Test Acc: 0.9316, ones: 0.20000000000000026\n",
            "Epoch: 100, Train Acc: 0.9971, Test Acc: 0.9316, ones: 0.20000000000000026\n",
            "Epoch: 101, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 102, Train Acc: 1.0000, Test Acc: 0.9316, ones: 0.20000000000000026\n",
            "Epoch: 103, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 104, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 105, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 106, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 107, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 108, Train Acc: 1.0000, Test Acc: 0.9316, ones: 0.20000000000000026\n",
            "Epoch: 109, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 110, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 111, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 112, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 113, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 114, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 115, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 116, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 117, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 118, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 119, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 120, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 121, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 122, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 123, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 124, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 125, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 126, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 127, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 128, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 129, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 130, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 131, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 132, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 133, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 134, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 135, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 136, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 137, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 138, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 139, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 140, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 141, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 142, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 143, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 144, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 145, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 146, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 147, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 148, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 149, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 150, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 151, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 152, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 153, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 154, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 155, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 156, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 157, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 158, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 159, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 160, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 161, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 162, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 163, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 164, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 165, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 166, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 167, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 168, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 169, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 170, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 171, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 172, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 173, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 174, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 175, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 176, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 177, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 178, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 179, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 180, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 181, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 182, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 183, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 184, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 185, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 186, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 187, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 188, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 189, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 190, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 191, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 192, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 193, Train Acc: 1.0000, Test Acc: 0.9402, ones: 0.29999999999999993\n",
            "Epoch: 194, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 195, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 196, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 197, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 198, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n",
            "Epoch: 199, Train Acc: 1.0000, Test Acc: 0.9487, ones: 0.3999999999999996\n"
          ]
        }
      ],
      "source": [
        "# Set display\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "# Call the model\n",
        "model = GCN(hidden_channels=64)\n",
        "\n",
        "# Set optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Set criterion\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the train function\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "         loss = criterion(out, data.y)  # Compute the loss.\n",
        "         loss.backward()  # Derive gradients.\n",
        "         optimizer.step()  # Update parameters based on gradients.\n",
        "         optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "# Define the test function\n",
        "def test(loader):\n",
        "     model.eval()\n",
        "\n",
        "     correct = 0\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "         out = model(data.x, data.edge_index, data.batch)  \n",
        "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "        #  print(pred)\n",
        "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "# Set the amount of epochs and run the GNN\n",
        "n_epochs = 200\n",
        "t_acc = []\n",
        "for epoch in range(1, n_epochs):\n",
        "    train()\n",
        "    # Get the train and test accuracy\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    # Calculate the percentage of correctly predicted y=1 for the test dataset\n",
        "    bad = ((1-test_acc)*len_td)\n",
        "    ones = (cnt-bad)/cnt\n",
        "    # Print output\n",
        "    print(f\"Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, perc correct y=1: {ones}\")\n",
        "    t_acc.append(ones)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rIUf99Mx39qc",
        "_BtdxFNP1cW0",
        "aMLMtL6H1oay"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('GNN')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "0738a566eb3943a9001d0b87275707c0bb8fdd2f63a0165cae641b428516525a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
